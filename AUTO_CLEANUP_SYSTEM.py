#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AUTO CLEANUP SYSTEM - р╕гр╕░р╕Ър╕Ър╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Фр╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░р╕нр╕▒р╕Хр╣Вр╕Щр╕бр╕▒р╕Хр╕┤
р╕кр╕гр╣Йр╕▓р╕Зр╣Ар╕бр╕╖р╣Ир╕н: 2025-07-04 23:47
р╕гр╕░р╕Фр╕▒р╕Ър╕Др╕зр╕▓р╕бр╕кр╕│р╕Др╕▒р╕Н: р╕кр╕╣р╕Зр╕кр╕╕р╕Ф

р╕гр╕░р╕Ър╕Ър╕Щр╕╡р╣Йр╕Ир╕░:
1. р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░
2. р╣Ар╕Бр╣Зр╕Ър╕Вр╣Йр╕нр╕бр╕╣р╕ер╕кр╕│р╕Др╕▒р╕Н
3. р╕ер╕Ър╣Др╕Яр╕ер╣Мр╕Чр╕╡р╣Ир╣Др╕бр╣Ир╕Ир╕│р╣Ар╕Ыр╣Зр╕Щ
4. р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╕Бр╕▓р╕гр╕Чр╕│р╕Зр╕▓р╕Щ
"""

import os
import json
import shutil
import logging
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# р╕Хр╕▒р╣Йр╕Зр╕Др╣Ир╕▓ logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('auto_cleanup.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AutoCleanupSystem:
    """р╕гр╕░р╕Ър╕Ър╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Фр╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░р╕нр╕▒р╕Хр╣Вр╕Щр╕бр╕▒р╕Хр╕┤"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.backup_dir = self.project_root / "backups" / "auto_cleanup"
        self.analysis_file = self.project_root / "cleanup_analysis.json"
        self.important_patterns = [
            "credentials", "config", "settings", "api_key", "token",
            "password", "secret", "private", "sensitive"
        ]
        self.junk_patterns = [
            "temp_", "debug_", "test_", "*.log", "*.tmp", "*.bak",
            "__pycache__", ".pytest_cache", "node_modules"
        ]
        
        # р╕кр╕гр╣Йр╕▓р╕Зр╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣М backup
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
    def analyze_files(self) -> Dict:
        """р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Др╕Яр╕ер╣Мр╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Фр╣Гр╕Щр╣Вр╕Ыр╕гр╣Ар╕Ир╕Бр╕Хр╣М"""
        logger.info("ЁЯФН р╣Ар╕гр╕┤р╣Ир╕бр╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Др╕Яр╕ер╣М...")
        
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "total_files": 0,
            "important_files": [],
            "junk_files": [],
            "large_files": [],
            "duplicate_files": [],
            "analysis_summary": {}
        }
        
        # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Др╕Яр╕ер╣Мр╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file():
                analysis["total_files"] += 1
                
                # р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╣Др╕Яр╕ер╣Мр╕кр╕│р╕Др╕▒р╕Н
                if self._is_important_file(file_path):
                    analysis["important_files"].append(str(file_path))
                
                # р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░
                if self._is_junk_file(file_path):
                    analysis["junk_files"].append(str(file_path))
                
                # р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╣Др╕Яр╕ер╣Мр╕Вр╕Щр╕▓р╕Фр╣Гр╕лр╕Нр╣И
                if file_path.stat().st_size > 10 * 1024 * 1024:  # > 10MB
                    analysis["large_files"].append({
                        "path": str(file_path),
                        "size_mb": file_path.stat().st_size / (1024 * 1024)
                    })
        
        # р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╣Др╕Яр╕ер╣Мр╕Лр╣Йр╕│
        analysis["duplicate_files"] = self._find_duplicates()
        
        # р╕кр╕гр╕╕р╕Ыр╕Бр╕▓р╕гр╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣М
        analysis["analysis_summary"] = {
            "important_count": len(analysis["important_files"]),
            "junk_count": len(analysis["junk_files"]),
            "large_count": len(analysis["large_files"]),
            "duplicate_count": len(analysis["duplicate_files"]),
            "estimated_cleanup_size_mb": self._calculate_cleanup_size(analysis)
        }
        
        # р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╕Бр╕▓р╕гр╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣М
        with open(self.analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        logger.info(f"тЬЕ р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Ар╕кр╕гр╣Зр╕Ир╕кр╕┤р╣Йр╕Щ: {analysis['total_files']} р╣Др╕Яр╕ер╣М")
        logger.info(f"ЁЯУК р╣Др╕Яр╕ер╣Мр╕кр╕│р╕Др╕▒р╕Н: {analysis['analysis_summary']['important_count']}")
        logger.info(f"ЁЯЧСя╕П р╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░: {analysis['analysis_summary']['junk_count']}")
        logger.info(f"ЁЯТ╛ р╕Вр╕Щр╕▓р╕Фр╕Чр╕╡р╣Ир╕Ыр╕гр╕░р╕лр╕вр╕▒р╕Фр╣Др╕Фр╣Й: {analysis['analysis_summary']['estimated_cleanup_size_mb']:.2f} MB")
        
        return analysis
    
    def _is_important_file(self, file_path: Path) -> bool:
        """р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕зр╣Ир╕▓р╣Ар╕Ыр╣Зр╕Щр╣Др╕Яр╕ер╣Мр╕кр╕│р╕Др╕▒р╕Нр╕лр╕гр╕╖р╕нр╣Др╕бр╣И"""
        file_name = file_path.name.lower()
        file_content = ""
        
        # р╕нр╣Ир╕▓р╕Щр╣Ар╕Щр╕╖р╣Йр╕нр╕лр╕▓р╣Др╕Яр╕ер╣М (р╣Ар╕Йр╕Юр╕▓р╕░р╣Др╕Яр╕ер╣Мр╕Вр╣Йр╕нр╕Др╕зр╕▓р╕б)
        if file_path.suffix in ['.txt', '.json', '.py', '.md', '.yml', '.yaml', '.env']:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    file_content = f.read().lower()
            except:
                pass
        
        # р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕Кр╕╖р╣Ир╕нр╣Др╕Яр╕ер╣М
        for pattern in self.important_patterns:
            if pattern in file_name:
                return True
        
        # р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╣Ар╕Щр╕╖р╣Йр╕нр╕лр╕▓р╣Др╕Яр╕ер╣М
        for pattern in self.important_patterns:
            if pattern in file_content:
                return True
        
        return False
    
    def _is_junk_file(self, file_path: Path) -> bool:
        """р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕зр╣Ир╕▓р╣Ар╕Ыр╣Зр╕Щр╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░р╕лр╕гр╕╖р╕нр╣Др╕бр╣И"""
        file_name = file_path.name.lower()
        
        # р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ pattern р╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░
        for pattern in self.junk_patterns:
            if pattern.startswith("*"):
                if file_name.endswith(pattern[1:]):
                    return True
            elif pattern in file_name:
                return True
        
        # р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣Мр╕Вр╕вр╕░
        if file_path.is_dir():
            if file_name in ["__pycache__", ".pytest_cache", "node_modules", "logs", "temp"]:
                return True
        
        return False
    
    def _find_duplicates(self) -> List[Dict]:
        """р╕лр╕▓р╕Яр╕┤р╕ер╣Мр╕Лр╣Йр╕│"""
        duplicates = []
        file_hashes = {}
        
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file() and file_path.stat().st_size > 1024:  # > 1KB
                try:
                    with open(file_path, 'rb') as f:
                        file_hash = hashlib.md5(f.read()).hexdigest()
                    
                    if file_hash in file_hashes:
                        duplicates.append({
                            "hash": file_hash,
                            "files": [str(file_hashes[file_hash]), str(file_path)]
                        })
                    else:
                        file_hashes[file_hash] = file_path
                except:
                    pass
        
        return duplicates
    
    def _calculate_cleanup_size(self, analysis: Dict) -> float:
        """р╕Др╕│р╕Щр╕зр╕Ур╕Вр╕Щр╕▓р╕Фр╕Чр╕╡р╣Ир╕Ыр╕гр╕░р╕лр╕вр╕▒р╕Фр╣Др╕Фр╣Й"""
        total_size = 0
        
        for file_path in analysis["junk_files"]:
            try:
                total_size += Path(file_path).stat().st_size
            except:
                pass
        
        return total_size / (1024 * 1024)  # MB
    
    def backup_important_files(self) -> bool:
        """р╕кр╕│р╕гр╕нр╕Зр╣Др╕Яр╕ер╣Мр╕кр╕│р╕Др╕▒р╕Н"""
        logger.info("ЁЯТ╛ р╣Ар╕гр╕┤р╣Ир╕бр╕кр╕│р╕гр╕нр╕Зр╣Др╕Яр╕ер╣Мр╕кр╕│р╕Др╕▒р╕Н...")
        
        try:
            # р╕нр╣Ир╕▓р╕Щр╕Бр╕▓р╕гр╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣М
            with open(self.analysis_file, 'r', encoding='utf-8') as f:
                analysis = json.load(f)
            
            backup_count = 0
            for file_path in analysis["important_files"]:
                try:
                    source_path = Path(file_path)
                    if source_path.exists():
                        # р╕кр╕гр╣Йр╕▓р╕Зр╣Вр╕Др╕гр╕Зр╕кр╕гр╣Йр╕▓р╕Зр╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣Мр╣Гр╕Щ backup
                        relative_path = source_path.relative_to(self.project_root)
                        backup_path = self.backup_dir / relative_path
                        backup_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        # р╕Др╕▒р╕Фр╕ер╕нр╕Бр╣Др╕Яр╕ер╣М
                        shutil.copy2(source_path, backup_path)
                        backup_count += 1
                        logger.debug(f"тЬЕ р╕кр╕│р╕гр╕нр╕З: {file_path}")
                except Exception as e:
                    logger.warning(f"тЪая╕П р╣Др╕бр╣Ир╕кр╕▓р╕бр╕▓р╕гр╕Цр╕кр╕│р╕гр╕нр╕З {file_path}: {e}")
            
            logger.info(f"тЬЕ р╕кр╕│р╕гр╕нр╕Зр╣Ар╕кр╕гр╣Зр╕Ир╕кр╕┤р╣Йр╕Щ: {backup_count} р╣Др╕Яр╕ер╣М")
            return True
            
        except Exception as e:
            logger.error(f"тЭМ р╣Ар╕Бр╕┤р╕Фр╕Вр╣Йр╕нр╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Фр╣Гр╕Щр╕Бр╕▓р╕гр╕кр╕│р╕гр╕нр╕З: {e}")
            return False
    
    def cleanup_junk_files(self, dry_run: bool = True) -> Dict:
        """р╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Фр╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░"""
        logger.info(f"ЁЯз╣ р╣Ар╕гр╕┤р╣Ир╕бр╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Ф (dry_run: {dry_run})...")
        
        cleanup_result = {
            "timestamp": datetime.now().isoformat(),
            "dry_run": dry_run,
            "removed_files": [],
            "removed_dirs": [],
            "errors": [],
            "total_size_saved_mb": 0
        }
        
        try:
            # р╕нр╣Ир╕▓р╕Щр╕Бр╕▓р╕гр╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣М
            with open(self.analysis_file, 'r', encoding='utf-8') as f:
                analysis = json.load(f)
            
            # р╕ер╕Ър╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░
            for file_path in analysis["junk_files"]:
                try:
                    path = Path(file_path)
                    if path.exists():
                        file_size = path.stat().st_size / (1024 * 1024)  # MB
                        
                        if not dry_run:
                            path.unlink()
                            cleanup_result["removed_files"].append({
                                "path": str(path),
                                "size_mb": file_size
                            })
                            cleanup_result["total_size_saved_mb"] += file_size
                            logger.info(f"ЁЯЧСя╕П р╕ер╕Ър╣Др╕Яр╕ер╣М: {path}")
                        else:
                            cleanup_result["removed_files"].append({
                                "path": str(path),
                                "size_mb": file_size
                            })
                            cleanup_result["total_size_saved_mb"] += file_size
                            logger.info(f"ЁЯФН р╕Ир╕░р╕ер╕Ър╣Др╕Яр╕ер╣М: {path}")
                except Exception as e:
                    cleanup_result["errors"].append({
                        "path": file_path,
                        "error": str(e)
                    })
                    logger.warning(f"тЪая╕П р╣Др╕бр╣Ир╕кр╕▓р╕бр╕▓р╕гр╕Цр╕ер╕Ъ {file_path}: {e}")
            
            # р╕ер╕Ър╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣Мр╕Вр╕вр╕░
            junk_dirs = ["__pycache__", ".pytest_cache", "node_modules", "logs", "temp"]
            for dir_name in junk_dirs:
                for dir_path in self.project_root.rglob(dir_name):
                    if dir_path.is_dir():
                        try:
                            dir_size = self._get_dir_size(dir_path) / (1024 * 1024)  # MB
                            
                            if not dry_run:
                                shutil.rmtree(dir_path)
                                cleanup_result["removed_dirs"].append({
                                    "path": str(dir_path),
                                    "size_mb": dir_size
                                })
                                cleanup_result["total_size_saved_mb"] += dir_size
                                logger.info(f"ЁЯЧСя╕П р╕ер╕Ър╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣М: {dir_path}")
                            else:
                                cleanup_result["removed_dirs"].append({
                                    "path": str(dir_path),
                                    "size_mb": dir_size
                                })
                                cleanup_result["total_size_saved_mb"] += dir_size
                                logger.info(f"ЁЯФН р╕Ир╕░р╕ер╕Ър╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣М: {dir_path}")
                        except Exception as e:
                            cleanup_result["errors"].append({
                                "path": str(dir_path),
                                "error": str(e)
                            })
                            logger.warning(f"тЪая╕П р╣Др╕бр╣Ир╕кр╕▓р╕бр╕▓р╕гр╕Цр╕ер╕Ър╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣М {dir_path}: {e}")
            
            # р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╕Ьр╕ер╕ер╕▒р╕Юр╕Шр╣М
            result_file = self.project_root / f"cleanup_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(cleanup_result, f, indent=2, ensure_ascii=False)
            
            logger.info(f"тЬЕ р╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Фр╣Ар╕кр╕гр╣Зр╕Ир╕кр╕┤р╣Йр╕Щ")
            logger.info(f"ЁЯУК р╣Др╕Яр╕ер╣Мр╕Чр╕╡р╣Ир╕ер╕Ъ: {len(cleanup_result['removed_files'])}")
            logger.info(f"ЁЯУБ р╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣Мр╕Чр╕╡р╣Ир╕ер╕Ъ: {len(cleanup_result['removed_dirs'])}")
            logger.info(f"ЁЯТ╛ р╕Вр╕Щр╕▓р╕Фр╕Чр╕╡р╣Ир╕Ыр╕гр╕░р╕лр╕вр╕▒р╕Фр╣Др╕Фр╣Й: {cleanup_result['total_size_saved_mb']:.2f} MB")
            
            return cleanup_result
            
        except Exception as e:
            logger.error(f"тЭМ р╣Ар╕Бр╕┤р╕Фр╕Вр╣Йр╕нр╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Фр╣Гр╕Щр╕Бр╕▓р╕гр╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Ф: {e}")
            return cleanup_result
    
    def _get_dir_size(self, dir_path: Path) -> int:
        """р╕Др╕│р╕Щр╕зр╕Ур╕Вр╕Щр╕▓р╕Фр╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣М"""
        total_size = 0
        try:
            for file_path in dir_path.rglob("*"):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
        except:
            pass
        return total_size
    
    def generate_report(self) -> str:
        """р╕кр╕гр╣Йр╕▓р╕Зр╕гр╕▓р╕вр╕Зр╕▓р╕Щр╕Бр╕▓р╕гр╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Ф"""
        logger.info("ЁЯУК р╕кр╕гр╣Йр╕▓р╕Зр╕гр╕▓р╕вр╕Зр╕▓р╕Щ...")
        
        try:
            # р╕нр╣Ир╕▓р╕Щр╕Бр╕▓р╕гр╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣М
            with open(self.analysis_file, 'r', encoding='utf-8') as f:
                analysis = json.load(f)
            
            report = f"""
# ЁЯз╣ AUTO CLEANUP REPORT
р╕кр╕гр╣Йр╕▓р╕Зр╣Ар╕бр╕╖р╣Ир╕н: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ЁЯУК р╕кр╕гр╕╕р╕Ыр╕Бр╕▓р╕гр╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣М
- р╣Др╕Яр╕ер╣Мр╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф: {analysis['total_files']:,} р╣Др╕Яр╕ер╣М
- р╣Др╕Яр╕ер╣Мр╕кр╕│р╕Др╕▒р╕Н: {analysis['analysis_summary']['important_count']} р╣Др╕Яр╕ер╣М
- р╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░: {analysis['analysis_summary']['junk_count']} р╣Др╕Яр╕ер╣М
- р╣Др╕Яр╕ер╣Мр╕Вр╕Щр╕▓р╕Фр╣Гр╕лр╕Нр╣И: {analysis['analysis_summary']['large_count']} р╣Др╕Яр╕ер╣М
- р╣Др╕Яр╕ер╣Мр╕Лр╣Йр╕│: {analysis['analysis_summary']['duplicate_count']} р╣Др╕Яр╕ер╣М

## ЁЯТ╛ р╕Ыр╕гр╕░р╣Вр╕вр╕Кр╕Щр╣Мр╕Чр╕╡р╣Ир╣Др╕Фр╣Й
- р╕Вр╕Щр╕▓р╕Фр╕Чр╕╡р╣Ир╕Ыр╕гр╕░р╕лр╕вр╕▒р╕Фр╣Др╕Фр╣Й: {analysis['analysis_summary']['estimated_cleanup_size_mb']:.2f} MB
- р╣Др╕Яр╕ер╣Мр╕Чр╕╡р╣Ир╕Ыр╕гр╕░р╕лр╕вр╕▒р╕Фр╣Др╕Фр╣Й: {analysis['analysis_summary']['junk_count']} р╣Др╕Яр╕ер╣М

## ЁЯФН р╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░р╕Чр╕╡р╣Ир╕Юр╕Ъ
"""
            
            for file_path in analysis["junk_files"][:20]:  # р╣Бр╕кр╕Фр╕З 20 р╣Др╕Яр╕ер╣Мр╣Бр╕гр╕Б
                report += f"- {file_path}\n"
            
            if len(analysis["junk_files"]) > 20:
                report += f"- ... р╣Бр╕ер╕░р╕нр╕╡р╕Б {len(analysis['junk_files']) - 20} р╣Др╕Яр╕ер╣М\n"
            
            report += f"""
## ЁЯУБ р╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣Мр╕Вр╕вр╕░р╕Чр╕╡р╣Ир╕Юр╕Ъ
"""
            
            junk_dirs = ["__pycache__", ".pytest_cache", "node_modules", "logs", "temp"]
            for dir_name in junk_dirs:
                dir_count = len(list(self.project_root.rglob(dir_name)))
                if dir_count > 0:
                    report += f"- {dir_name}: {dir_count} р╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣М\n"
            
            report += f"""
## ЁЯОп р╣Бр╕Щр╕░р╕Щр╕│
1. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╣Др╕Яр╕ер╣Мр╕кр╕│р╕Др╕▒р╕Нр╕Бр╣Ир╕нр╕Щр╕ер╕Ъ
2. р╣Гр╕Кр╣Й dry_run=True р╕Бр╣Ир╕нр╕Щр╕ер╕Ър╕Ир╕гр╕┤р╕З
3. р╕кр╕│р╕гр╕нр╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Бр╣Ир╕нр╕Щр╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Ф
4. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕Ьр╕ер╕ер╕▒р╕Юр╕Шр╣Мр╕лр╕ер╕▒р╕Зр╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Ф

---
р╕кр╕гр╣Йр╕▓р╕Зр╣Вр╕Фр╕в: WAWAGOT.AI Auto Cleanup System
"""
            
            # р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╕гр╕▓р╕вр╕Зр╕▓р╕Щ
            report_file = self.project_root / f"cleanup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"тЬЕ р╕кр╕гр╣Йр╕▓р╕Зр╕гр╕▓р╕вр╕Зр╕▓р╕Щр╣Ар╕кр╕гр╣Зр╕Ир╕кр╕┤р╣Йр╕Щ: {report_file}")
            return str(report_file)
            
        except Exception as e:
            logger.error(f"тЭМ р╣Ар╕Бр╕┤р╕Фр╕Вр╣Йр╕нр╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Фр╣Гр╕Щр╕Бр╕▓р╕гр╕кр╕гр╣Йр╕▓р╕Зр╕гр╕▓р╕вр╕Зр╕▓р╕Щ: {e}")
            return ""

def main():
    """р╕Яр╕▒р╕Зр╕Бр╣Мр╕Кр╕▒р╕Щр╕лр╕ер╕▒р╕Б"""
    logger.info("ЁЯЪА р╣Ар╕гр╕┤р╣Ир╕бр╕Хр╣Йр╕Щ Auto Cleanup System")
    
    try:
        # р╕кр╕гр╣Йр╕▓р╕Зр╕гр╕░р╕Ър╕Ъ
        cleanup_system = AutoCleanupSystem()
        
        # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Др╕Яр╕ер╣М
        analysis = cleanup_system.analyze_files()
        
        # р╕кр╕гр╣Йр╕▓р╕Зр╕гр╕▓р╕вр╕Зр╕▓р╕Щ
        report_file = cleanup_system.generate_report()
        
        # р╕кр╕│р╕гр╕нр╕Зр╣Др╕Яр╕ер╣Мр╕кр╕│р╕Др╕▒р╕Н
        if analysis["analysis_summary"]["important_count"] > 0:
            backup_success = cleanup_system.backup_important_files()
            if not backup_success:
                logger.warning("тЪая╕П р╕Бр╕▓р╕гр╕кр╕│р╕гр╕нр╕Зр╣Др╕бр╣Ир╕кр╕│р╣Ар╕гр╣Зр╕И - р╕лр╕вр╕╕р╕Фр╕Бр╕▓р╕гр╕Чр╕│р╕Зр╕▓р╕Щ")
                return
        
        # р╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Ф (dry run)
        logger.info("ЁЯз╣ р╣Ар╕гр╕┤р╣Ир╕бр╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Ф (dry run)...")
        cleanup_result = cleanup_system.cleanup_junk_files(dry_run=True)
        
        # р╣Бр╕кр╕Фр╕Зр╕Ьр╕ер╕ер╕▒р╕Юр╕Шр╣М
        print(f"\n{'='*60}")
        print("ЁЯз╣ AUTO CLEANUP SYSTEM - р╕Ьр╕ер╕ер╕▒р╕Юр╕Шр╣М")
        print(f"{'='*60}")
        print(f"ЁЯУК р╣Др╕Яр╕ер╣Мр╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф: {analysis['total_files']:,}")
        print(f"ЁЯЧСя╕П р╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░: {analysis['analysis_summary']['junk_count']}")
        print(f"ЁЯТ╛ р╕Вр╕Щр╕▓р╕Фр╕Чр╕╡р╣Ир╕Ыр╕гр╕░р╕лр╕вр╕▒р╕Фр╣Др╕Фр╣Й: {analysis['analysis_summary']['estimated_cleanup_size_mb']:.2f} MB")
        print(f"ЁЯУД р╕гр╕▓р╕вр╕Зр╕▓р╕Щ: {report_file}")
        print(f"{'='*60}")
        
        # р╕Цр╕▓р╕бр╕Ьр╕╣р╣Йр╣Гр╕Кр╣Йр╕зр╣Ир╕▓р╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕гр╕ер╕Ър╕Ир╕гр╕┤р╕Зр╕лр╕гр╕╖р╕нр╣Др╕бр╣И
        if analysis["analysis_summary"]["junk_count"] > 0:
            print(f"\nтЭУ р╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕гр╕ер╕Ър╣Др╕Яр╕ер╣Мр╕Вр╕вр╕░р╕Ир╕гр╕┤р╕Зр╕лр╕гр╕╖р╕нр╣Др╕бр╣И? (y/N): ", end="")
            response = input().strip().lower()
            
            if response == 'y':
                logger.info("ЁЯз╣ р╣Ар╕гр╕┤р╣Ир╕бр╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Фр╕Ир╕гр╕┤р╕З...")
                cleanup_result = cleanup_system.cleanup_junk_files(dry_run=False)
                print(f"тЬЕ р╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Фр╣Ар╕кр╕гр╣Зр╕Ир╕кр╕┤р╣Йр╕Щ!")
                print(f"ЁЯТ╛ р╕Ыр╕гр╕░р╕лр╕вр╕▒р╕Фр╕Юр╕╖р╣Йр╕Щр╕Чр╕╡р╣И: {cleanup_result['total_size_saved_mb']:.2f} MB")
            else:
                print("тЭМ р╕вр╕Бр╣Ар╕ер╕┤р╕Бр╕Бр╕▓р╕гр╕Чр╕│р╕Др╕зр╕▓р╕бр╕кр╕░р╕нр╕▓р╕Ф")
        
        logger.info("ЁЯОЙ Auto Cleanup System р╣Ар╕кр╕гр╣Зр╕Ир╕кр╕┤р╣Йр╕Щ")
        
    except KeyboardInterrupt:
        logger.info("ЁЯЫС р╕Цр╕╣р╕Бр╕лр╕вр╕╕р╕Фр╣Вр╕Фр╕вр╕Ьр╕╣р╣Йр╣Гр╕Кр╣Й")
    except Exception as e:
        logger.error(f"тЭМ р╣Ар╕Бр╕┤р╕Фр╕Вр╣Йр╕нр╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Ф: {e}")

if __name__ == "__main__":
    main() 