#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AUTO CLEANUP SYSTEM - ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠: 2025-07-04 23:47
‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î

‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏à‡∏∞:
1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞
2. ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
3. ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
"""

import os
import json
import shutil
import logging
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('auto_cleanup.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AutoCleanupSystem:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.backup_dir = self.project_root / "backups" / "auto_cleanup"
        self.analysis_file = self.project_root / "cleanup_analysis.json"
        self.important_patterns = [
            "credentials", "config", "settings", "api_key", "token",
            "password", "secret", "private", "sensitive"
        ]
        self.junk_patterns = [
            "temp_", "debug_", "test_", "*.log", "*.tmp", "*.bak",
            "__pycache__", ".pytest_cache", "node_modules"
        ]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå backup
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
    def analyze_files(self) -> Dict:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå"""
        logger.info("üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå...")
        
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "total_files": 0,
            "important_files": [],
            "junk_files": [],
            "large_files": [],
            "duplicate_files": [],
            "analysis_summary": {}
        }
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file():
                analysis["total_files"] += 1
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                if self._is_important_file(file_path):
                    analysis["important_files"].append(str(file_path))
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞
                if self._is_junk_file(file_path):
                    analysis["junk_files"].append(str(file_path))
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
                if file_path.stat().st_size > 10 * 1024 * 1024:  # > 10MB
                    analysis["large_files"].append({
                        "path": str(file_path),
                        "size_mb": file_path.stat().st_size / (1024 * 1024)
                    })
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥
        analysis["duplicate_files"] = self._find_duplicates()
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        analysis["analysis_summary"] = {
            "important_count": len(analysis["important_files"]),
            "junk_count": len(analysis["junk_files"]),
            "large_count": len(analysis["large_files"]),
            "duplicate_count": len(analysis["duplicate_files"]),
            "estimated_cleanup_size_mb": self._calculate_cleanup_size(analysis)
        }
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        with open(self.analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {analysis['total_files']} ‡πÑ‡∏ü‡∏•‡πå")
        logger.info(f"üìä ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {analysis['analysis_summary']['important_count']}")
        logger.info(f"üóëÔ∏è ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞: {analysis['analysis_summary']['junk_count']}")
        logger.info(f"üíæ ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÑ‡∏î‡πâ: {analysis['analysis_summary']['estimated_cleanup_size_mb']:.2f} MB")
        
        return analysis
    
    def _is_important_file(self, file_path: Path) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        file_name = file_path.name.lower()
        file_content = ""
        
        # ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)
        if file_path.suffix in ['.txt', '.json', '.py', '.md', '.yml', '.yaml', '.env']:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    file_content = f.read().lower()
            except:
                pass
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        for pattern in self.important_patterns:
            if pattern in file_name:
                return True
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå
        for pattern in self.important_patterns:
            if pattern in file_content:
                return True
        
        return False
    
    def _is_junk_file(self, file_path: Path) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        file_name = file_path.name.lower()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö pattern ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞
        for pattern in self.junk_patterns:
            if pattern.startswith("*"):
                if file_name.endswith(pattern[1:]):
                    return True
            elif pattern in file_name:
                return True
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡∏¢‡∏∞
        if file_path.is_dir():
            if file_name in ["__pycache__", ".pytest_cache", "node_modules", "logs", "temp"]:
                return True
        
        return False
    
    def _find_duplicates(self) -> List[Dict]:
        """‡∏´‡∏≤‡∏ü‡∏¥‡∏•‡πå‡∏ã‡πâ‡∏≥"""
        duplicates = []
        file_hashes = {}
        
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file() and file_path.stat().st_size > 1024:  # > 1KB
                try:
                    with open(file_path, 'rb') as f:
                        file_hash = hashlib.md5(f.read()).hexdigest()
                    
                    if file_hash in file_hashes:
                        duplicates.append({
                            "hash": file_hash,
                            "files": [str(file_hashes[file_hash]), str(file_path)]
                        })
                    else:
                        file_hashes[file_hash] = file_path
                except:
                    pass
        
        return duplicates
    
    def _calculate_cleanup_size(self, analysis: Dict) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÑ‡∏î‡πâ"""
        total_size = 0
        
        for file_path in analysis["junk_files"]:
            try:
                total_size += Path(file_path).stat().st_size
            except:
                pass
        
        return total_size / (1024 * 1024)  # MB
    
    def backup_important_files(self) -> bool:
        """‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"""
        logger.info("üíæ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç...")
        
        try:
            # ‡∏≠‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
            with open(self.analysis_file, 'r', encoding='utf-8') as f:
                analysis = json.load(f)
            
            backup_count = 0
            for file_path in analysis["important_files"]:
                try:
                    source_path = Path(file_path)
                    if source_path.exists():
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÉ‡∏ô backup
                        relative_path = source_path.relative_to(self.project_root)
                        backup_path = self.backup_dir / relative_path
                        backup_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        # ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå
                        shutil.copy2(source_path, backup_path)
                        backup_count += 1
                        logger.debug(f"‚úÖ ‡∏™‡∏≥‡∏£‡∏≠‡∏á: {file_path}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏≥‡∏£‡∏≠‡∏á {file_path}: {e}")
            
            logger.info(f"‚úÖ ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {backup_count} ‡πÑ‡∏ü‡∏•‡πå")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏≠‡∏á: {e}")
            return False
    
    def cleanup_junk_files(self, dry_run: bool = True) -> Dict:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞"""
        logger.info(f"üßπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î (dry_run: {dry_run})...")
        
        cleanup_result = {
            "timestamp": datetime.now().isoformat(),
            "dry_run": dry_run,
            "removed_files": [],
            "removed_dirs": [],
            "errors": [],
            "total_size_saved_mb": 0
        }
        
        try:
            # ‡∏≠‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
            with open(self.analysis_file, 'r', encoding='utf-8') as f:
                analysis = json.load(f)
            
            # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞
            for file_path in analysis["junk_files"]:
                try:
                    path = Path(file_path)
                    if path.exists():
                        file_size = path.stat().st_size / (1024 * 1024)  # MB
                        
                        if not dry_run:
                            path.unlink()
                            cleanup_result["removed_files"].append({
                                "path": str(path),
                                "size_mb": file_size
                            })
                            cleanup_result["total_size_saved_mb"] += file_size
                            logger.info(f"üóëÔ∏è ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå: {path}")
                        else:
                            cleanup_result["removed_files"].append({
                                "path": str(path),
                                "size_mb": file_size
                            })
                            cleanup_result["total_size_saved_mb"] += file_size
                            logger.info(f"üîç ‡∏à‡∏∞‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå: {path}")
                except Exception as e:
                    cleanup_result["errors"].append({
                        "path": file_path,
                        "error": str(e)
                    })
                    logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö {file_path}: {e}")
            
            # ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡∏¢‡∏∞
            junk_dirs = ["__pycache__", ".pytest_cache", "node_modules", "logs", "temp"]
            for dir_name in junk_dirs:
                for dir_path in self.project_root.rglob(dir_name):
                    if dir_path.is_dir():
                        try:
                            dir_size = self._get_dir_size(dir_path) / (1024 * 1024)  # MB
                            
                            if not dry_run:
                                shutil.rmtree(dir_path)
                                cleanup_result["removed_dirs"].append({
                                    "path": str(dir_path),
                                    "size_mb": dir_size
                                })
                                cleanup_result["total_size_saved_mb"] += dir_size
                                logger.info(f"üóëÔ∏è ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {dir_path}")
                            else:
                                cleanup_result["removed_dirs"].append({
                                    "path": str(dir_path),
                                    "size_mb": dir_size
                                })
                                cleanup_result["total_size_saved_mb"] += dir_size
                                logger.info(f"üîç ‡∏à‡∏∞‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {dir_path}")
                        except Exception as e:
                            cleanup_result["errors"].append({
                                "path": str(dir_path),
                                "error": str(e)
                            })
                            logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå {dir_path}: {e}")
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            result_file = self.project_root / f"cleanup_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(cleanup_result, f, indent=2, ensure_ascii=False)
            
            logger.info(f"‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
            logger.info(f"üìä ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏•‡∏ö: {len(cleanup_result['removed_files'])}")
            logger.info(f"üìÅ ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏•‡∏ö: {len(cleanup_result['removed_dirs'])}")
            logger.info(f"üíæ ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÑ‡∏î‡πâ: {cleanup_result['total_size_saved_mb']:.2f} MB")
            
            return cleanup_result
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î: {e}")
            return cleanup_result
    
    def _get_dir_size(self, dir_path: Path) -> int:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå"""
        total_size = 0
        try:
            for file_path in dir_path.rglob("*"):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
        except:
            pass
        return total_size
    
    def generate_report(self) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î"""
        logger.info("üìä ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô...")
        
        try:
            # ‡∏≠‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
            with open(self.analysis_file, 'r', encoding='utf-8') as f:
                analysis = json.load(f)
            
            report = f"""
# üßπ AUTO CLEANUP REPORT
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
- ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {analysis['total_files']:,} ‡πÑ‡∏ü‡∏•‡πå
- ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {analysis['analysis_summary']['important_count']} ‡πÑ‡∏ü‡∏•‡πå
- ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞: {analysis['analysis_summary']['junk_count']} ‡πÑ‡∏ü‡∏•‡πå
- ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà: {analysis['analysis_summary']['large_count']} ‡πÑ‡∏ü‡∏•‡πå
- ‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥: {analysis['analysis_summary']['duplicate_count']} ‡πÑ‡∏ü‡∏•‡πå

## üíæ ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ
- ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÑ‡∏î‡πâ: {analysis['analysis_summary']['estimated_cleanup_size_mb']:.2f} MB
- ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÑ‡∏î‡πâ: {analysis['analysis_summary']['junk_count']} ‡πÑ‡∏ü‡∏•‡πå

## üîç ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏û‡∏ö
"""
            
            for file_path in analysis["junk_files"][:20]:  # ‡πÅ‡∏™‡∏î‡∏á 20 ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å
                report += f"- {file_path}\n"
            
            if len(analysis["junk_files"]) > 20:
                report += f"- ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(analysis['junk_files']) - 20} ‡πÑ‡∏ü‡∏•‡πå\n"
            
            report += f"""
## üìÅ ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏û‡∏ö
"""
            
            junk_dirs = ["__pycache__", ".pytest_cache", "node_modules", "logs", "temp"]
            for dir_name in junk_dirs:
                dir_count = len(list(self.project_root.rglob(dir_name)))
                if dir_count > 0:
                    report += f"- {dir_name}: {dir_count} ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå\n"
            
            report += f"""
## üéØ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö
2. ‡πÉ‡∏ä‡πâ dry_run=True ‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö‡∏à‡∏£‡∏¥‡∏á
3. ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î

---
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢: WAWAGOT.AI Auto Cleanup System
"""
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
            report_file = self.project_root / f"cleanup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {report_file}")
            return str(report_file)
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô: {e}")
            return ""

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    logger.info("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Auto Cleanup System")
    
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö
        cleanup_system = AutoCleanupSystem()
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå
        analysis = cleanup_system.analyze_files()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        report_file = cleanup_system.generate_report()
        
        # ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        if analysis["analysis_summary"]["important_count"] > 0:
            backup_success = cleanup_system.backup_important_files()
            if not backup_success:
                logger.warning("‚ö†Ô∏è ‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à - ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
                return
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î (dry run)
        logger.info("üßπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î (dry run)...")
        cleanup_result = cleanup_system.cleanup_junk_files(dry_run=True)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        print(f"\n{'='*60}")
        print("üßπ AUTO CLEANUP SYSTEM - ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
        print(f"{'='*60}")
        print(f"üìä ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {analysis['total_files']:,}")
        print(f"üóëÔ∏è ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞: {analysis['analysis_summary']['junk_count']}")
        print(f"üíæ ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÑ‡∏î‡πâ: {analysis['analysis_summary']['estimated_cleanup_size_mb']:.2f} MB")
        print(f"üìÑ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô: {report_file}")
        print(f"{'='*60}")
        
        # ‡∏ñ‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if analysis["analysis_summary"]["junk_count"] > 0:
            print(f"\n‚ùì ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/N): ", end="")
            response = input().strip().lower()
            
            if response == 'y':
                logger.info("üßπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏à‡∏£‡∏¥‡∏á...")
                cleanup_result = cleanup_system.cleanup_junk_files(dry_run=False)
                print(f"‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
                print(f"üíæ ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà: {cleanup_result['total_size_saved_mb']:.2f} MB")
            else:
                print("‚ùå ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î")
        
        logger.info("üéâ Auto Cleanup System ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        
    except KeyboardInterrupt:
        logger.info("üõë ‡∏ñ‡∏π‡∏Å‡∏´‡∏¢‡∏∏‡∏î‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
    except Exception as e:
        logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")

if __name__ == "__main__":
    main() 