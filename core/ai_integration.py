"""
Multimodal AI Integration
‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö AI ‡πÅ‡∏ö‡∏ö Multimodal (Local Processing)
"""

import base64
from typing import Dict, Any, List
import cv2
import numpy as np
import json
import re
from pathlib import Path

class MultimodalAIIntegration:
    def __init__(self, api_key=None):
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô AI Integration (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ OpenAI API)"""
        self.api_key = api_key
        self.use_openai = api_key is not None and api_key.strip() != ""
        
        if self.use_openai:
            try:
                import openai
                self.client = openai.OpenAI(api_key=api_key)
                print("ü§ñ ‡πÉ‡∏ä‡πâ OpenAI API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI Integration")
            except Exception as e:
                print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ OpenAI API: {e}")
                self.use_openai = False
        
        if not self.use_openai:
            print("üß† ‡πÉ‡∏ä‡πâ Local AI Processing ‡πÅ‡∏ó‡∏ô OpenAI API")
        
    async def analyze_multimodal(self, images: List[str], text: str, prompt: str):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö Multimodal"""
        if self.use_openai:
            return await self._analyze_with_openai(images, text, prompt)
        else:
            return await self._analyze_local(images, text, prompt)
    
    async def _analyze_with_openai(self, images: List[str], text: str, prompt: str):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ OpenAI API"""
        try:
            content = [{"type": "text", "text": prompt + "\n\n" + text}]
            
            for image_path in images:
                with open(image_path, "rb") as image_file:
                    image_data = base64.b64encode(image_file.read()).decode('utf-8')
                    content.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
                    })
            
            response = self.client.chat.completions.create(
                model="gpt-4-vision-preview",
                messages=[{"role": "user", "content": content}],
                max_tokens=2000
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"‚ùå OpenAI API Error: {e}")
            return await self._analyze_local(images, text, prompt)
    
    async def _analyze_local(self, images: List[str], text: str, prompt: str):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö Local Processing"""
        analysis = {
            "prompt": prompt,
            "text_input": text,
            "images_analyzed": len(images),
            "analysis": "Local AI Processing - Basic Analysis",
            "confidence": 0.7
        }
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        for i, image_path in enumerate(images):
            img_analysis = await self._analyze_image_local(image_path)
            analysis[f"image_{i+1}"] = img_analysis
        
        return json.dumps(analysis, indent=2, ensure_ascii=False)
    
    async def _analyze_image_local(self, image_path: str):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö Local"""
        try:
            img = cv2.imread(image_path)
            if img is None:
                return {"error": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ"}
            
            height, width = img.shape[:2]
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            analysis = {
                "dimensions": f"{width}x{height}",
                "file_size": Path(image_path).stat().st_size,
                "file_type": Path(image_path).suffix,
                "analysis": "Local Image Analysis"
            }
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏™‡∏µ‡∏´‡∏•‡∏±‡∏Å
            if len(img.shape) == 3:  # Color image
                avg_color = np.mean(img, axis=(0, 1))
                analysis["dominant_colors"] = {
                    "blue": int(avg_color[0]),
                    "green": int(avg_color[1]), 
                    "red": int(avg_color[2])
                }
            
            return analysis
            
        except Exception as e:
            return {"error": f"‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}"}
        
    async def element_detection(self, screenshot_path, element_description):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö element ‡∏î‡πâ‡∏ß‡∏¢ AI"""
        if self.use_openai:
            result = await self._vision_analysis_openai(screenshot_path, element_description)
        else:
            result = await self._element_detection_local(screenshot_path, element_description)
        
        return self.parse_coordinates(result)
    
    async def _vision_analysis_openai(self, screenshot_path, prompt):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢ OpenAI"""
        try:
            with open(screenshot_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode('utf-8')
            
            response = self.client.chat.completions.create(
                model="gpt-4-vision-preview",
                messages=[{
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                    ]
                }],
                max_tokens=1000
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"‚ùå OpenAI Vision Error: {e}")
            return await self._element_detection_local(screenshot_path, prompt)
    
    async def _element_detection_local(self, screenshot_path, element_description):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö element ‡πÅ‡∏ö‡∏ö Local"""
        try:
            img = cv2.imread(screenshot_path)
            if img is None:
                return json.dumps({"error": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ"})
            
            height, width = img.shape[:2]
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö element ‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)
            elements = []
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏∏‡πà‡∏° (‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß/‡πÄ‡∏ó‡∏≤)
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            for contour in contours[:5]:  # 5 elements ‡πÅ‡∏£‡∏Å
                x, y, w, h = cv2.boundingRect(contour)
                if w > 50 and h > 20:  # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥
                    elements.append({
                        "type": "button",
                        "coordinates": [x + w//2, y + h//2],
                        "size": [w, h],
                        "confidence": 0.6
                    })
            
            return json.dumps({
                "description": element_description,
                "image_size": [width, height],
                "elements_found": elements,
                "method": "Local Detection"
            })
            
        except Exception as e:
            return json.dumps({"error": f"‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö element ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}"})
        
    def parse_coordinates(self, ai_response):
        """‡πÅ‡∏¢‡∏Å‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏à‡∏≤‡∏Å AI response"""
        try:
            if isinstance(ai_response, str):
                # ‡∏•‡∏≠‡∏á parse JSON
                data = json.loads(ai_response)
                
                # ‡∏´‡∏≤ coordinates
                if "elements_found" in data and data["elements_found"]:
                    return data["elements_found"][0]["coordinates"]
                elif "coordinates" in data:
                    return data["coordinates"]
                    
            # Fallback: ‡∏´‡∏≤ coordinates ‡∏à‡∏≤‡∏Å text
            coords_match = re.search(r'\[(\d+),\s*(\d+)\]', str(ai_response))
            if coords_match:
                return [int(coords_match.group(1)), int(coords_match.group(2))]
                
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏û‡∏¥‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ: {e}")
        
        return None

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° Missing Methods ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
    def process_text(self, text: str, task: str = "general") -> Dict[str, any]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Method ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ)"""
        try:
            result = {
                'input_text': text,
                'task': task,
                'processed_at': str(Path(__file__).parent / 'logs' / 'ai_processing.log'),
                'confidence': 0.8
            }
            
            if task == "sentiment":
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å
                positive_words = ['‡∏î‡∏µ', '‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°', '‡∏ä‡∏≠‡∏ö', '‡∏î‡∏µ‡πÉ‡∏à', '‡∏™‡∏∏‡∏Ç', 'good', 'great', 'love', 'happy']
                negative_words = ['‡πÅ‡∏¢‡πà', '‡πÑ‡∏°‡πà‡∏î‡∏µ', '‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à', '‡πÇ‡∏Å‡∏£‡∏ò', 'bad', 'terrible', 'hate', 'angry']
                
                text_lower = text.lower()
                positive_count = sum(1 for word in positive_words if word in text_lower)
                negative_count = sum(1 for word in negative_words if word in text_lower)
                
                if positive_count > negative_count:
                    result['sentiment'] = 'positive'
                    result['score'] = positive_count / len(text.split())
                elif negative_count > positive_count:
                    result['sentiment'] = 'negative'
                    result['score'] = negative_count / len(text.split())
                else:
                    result['sentiment'] = 'neutral'
                    result['score'] = 0.0
                    
            elif task == "keywords":
                # ‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                words = text.lower().split()
                from collections import Counter
                word_freq = Counter(words)
                result['keywords'] = [word for word, freq in word_freq.most_common(5)]
                
            elif task == "summary":
                # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                sentences = text.split('.')
                if len(sentences) > 1:
                    result['summary'] = sentences[0] + '.'
                else:
                    result['summary'] = text[:100] + '...'
                    
            else:
                # ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
                result['word_count'] = len(text.split())
                result['char_count'] = len(text)
                result['language'] = 'thai' if any('\u0e00' <= char <= '\u0e7f' for char in text) else 'english'
            
            return result
            
        except Exception as e:
            return {
                'error': f'‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}',
                'input_text': text,
                'task': task,
                'confidence': 0.0
            }
    
    def process_image(self, image_path: str, task: str = "analysis") -> Dict[str, any]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û (Method ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ)"""
        try:
            result = {
                'image_path': image_path,
                'task': task,
                'processed_at': str(Path(__file__).parent / 'logs' / 'image_processing.log'),
                'confidence': 0.7
            }
            
            img = cv2.imread(image_path)
            if img is None:
                return {'error': '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ', 'image_path': image_path}
            
            height, width = img.shape[:2]
            result['dimensions'] = {'width': width, 'height': height}
            result['file_size'] = Path(image_path).stat().st_size
            
            if task == "ocr":
                # OCR ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                import easyocr
                reader = easyocr.Reader(['th', 'en'])
                ocr_results = reader.readtext(image_path)
                result['text_detected'] = [text for _, text, conf in ocr_results if conf > 0.5]
                result['ocr_confidence'] = sum(conf for _, _, conf in ocr_results) / len(ocr_results) if ocr_results else 0
                
            elif task == "objects":
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)
                contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                result['objects_detected'] = len(contours)
                
            elif task == "colors":
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏µ
                if len(img.shape) == 3:
                    avg_color = np.mean(img, axis=(0, 1))
                    result['dominant_colors'] = {
                        'blue': int(avg_color[0]),
                        'green': int(avg_color[1]),
                        'red': int(avg_color[2])
                    }
                    
            else:
                # ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
                result['channels'] = img.shape[2] if len(img.shape) == 3 else 1
                result['aspect_ratio'] = width / height if height > 0 else 0
            
            return result
            
        except Exception as e:
            return {
                'error': f'‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}',
                'image_path': image_path,
                'task': task,
                'confidence': 0.0
            }
    
    def smart_analysis(self, data: Dict[str, any]) -> Dict[str, any]:
        """‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ (Method ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ)"""
        try:
            result = {
                'analysis_type': 'smart_analysis',
                'input_data': data,
                'processed_at': str(Path(__file__).parent / 'logs' / 'smart_analysis.log'),
                'confidence': 0.8
            }
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if 'text' in data:
                text_result = self.process_text(data['text'], 'general')
                result['text_analysis'] = text_result
                
            if 'image_path' in data:
                image_result = self.process_image(data['image_path'], 'analysis')
                result['image_analysis'] = image_result
                
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
            result['summary'] = {
                'total_analyses': len([k for k in result.keys() if k.endswith('_analysis')]),
                'overall_confidence': result.get('confidence', 0.0),
                'recommendations': self._generate_recommendations(result)
            }
            
            return result
            
        except Exception as e:
            return {
                'error': f'‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}',
                'input_data': data,
                'confidence': 0.0
            }
    
    def _generate_recommendations(self, analysis_result: Dict[str, any]) -> List[str]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"""
        recommendations = []
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô
        if analysis_result.get('confidence', 0) < 0.5:
            recommendations.append("‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥")
            
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        if 'text_analysis' in analysis_result:
            text_analysis = analysis_result['text_analysis']
            if text_analysis.get('word_count', 0) < 5:
                recommendations.append("‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô")
                
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        if 'image_analysis' in analysis_result:
            image_analysis = analysis_result['image_analysis']
            if image_analysis.get('dimensions', {}).get('width', 0) < 100:
                recommendations.append("‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô")
                
        if not recommendations:
            recommendations.append("‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠")
            
        return recommendations
