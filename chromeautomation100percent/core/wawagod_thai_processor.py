#!/usr/bin/env python3
"""
üáπüá≠ WAWAGOD Thai Language Processor - ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
‡∏£‡∏ß‡∏° PyThaiNLP, EasyOCR, ‡πÅ‡∏•‡∏∞ Thai Command Recognition
"""

import asyncio
import logging
import re
import json
from typing import Dict, Any, List, Optional
from datetime import datetime

class WAWAGODThaiProcessor:
    """
    üáπüá≠ WAWAGOD Thai Language Processor
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP ‡πÅ‡∏•‡∏∞ EasyOCR
    """
    
    def __init__(self):
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Thai Language Processor"""
        self.logger = logging.getLogger('WAWAGOD.ThaiProcessor')
        self.initialized = False
        
        # Thai NLP Components
        self.pythainlp = None
        self.easyocr = None
        self.thai_tokenizer = None
        
        # Thai Command Mappings
        self.command_mappings = {
            '‡πÄ‡∏õ‡∏¥‡∏î': 'open',
            '‡∏õ‡∏¥‡∏î': 'close',
            '‡∏Ñ‡∏•‡∏¥‡∏Å': 'click',
            '‡∏Å‡∏£‡∏≠‡∏Å': 'fill',
            '‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤': 'search',
            '‡∏£‡∏≠': 'wait',
            '‡∏ñ‡πà‡∏≤‡∏¢': 'screenshot',
            '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå': 'analyze',
            '‡∏ô‡∏≥‡∏ó‡∏≤‡∏á': 'navigate',
            '‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå': 'website',
            '‡∏õ‡∏∏‡πà‡∏°': 'button',
            '‡∏ü‡∏≠‡∏£‡πå‡∏°': 'form',
            '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°': 'text',
            '‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û': 'image',
            '‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠': 'screen'
        }
        
        # Thai Element Descriptions
        self.element_descriptions = {
            '‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤': 'search button',
            '‡∏ä‡πà‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤': 'search box',
            '‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö': 'login button',
            '‡∏ä‡πà‡∏≠‡∏á‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô': 'password field',
            '‡∏ä‡πà‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ': 'username field',
            '‡∏õ‡∏∏‡πà‡∏°‡∏™‡πà‡∏á': 'submit button',
            '‡∏•‡∏¥‡∏á‡∏Å‡πå': 'link',
            '‡πÄ‡∏°‡∏ô‡∏π': 'menu',
            '‡πÅ‡∏ó‡πá‡∏ö': 'tab'
        }
        
        self.logger.info("‚úÖ WAWAGOD Thai Language Processor ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")

    async def initialize(self):
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Thai Language Processor"""
        try:
            self.logger.info("üîß ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Thai Language Processor...")
            
            # ‡πÇ‡∏´‡∏•‡∏î PyThaiNLP ‡πÅ‡∏ö‡∏ö Parallel
            await self._load_pythainlp()
            
            # ‡πÇ‡∏´‡∏•‡∏î EasyOCR ‡πÅ‡∏ö‡∏ö Parallel
            await self._load_easyocr()
            
            # ‡πÇ‡∏´‡∏•‡∏î Thai Tokenizer ‡πÅ‡∏ö‡∏ö Parallel
            await self._load_thai_tokenizer()
            
            self.initialized = True
            self.logger.info("‚úÖ Thai Language Processor ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Thai Processor: {e}")
            return False

    async def _load_pythainlp(self):
        """‡πÇ‡∏´‡∏•‡∏î PyThaiNLP"""
        try:
            import pythainlp
            self.pythainlp = pythainlp
            self.logger.info("‚úÖ PyThaiNLP ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        except ImportError:
            self.logger.warning("‚ö†Ô∏è PyThaiNLP ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á")
            self.pythainlp = None

    async def _load_easyocr(self):
        """‡πÇ‡∏´‡∏•‡∏î EasyOCR"""
        try:
            import easyocr
            self.easyocr = easyocr.Reader(['th', 'en'])
            self.logger.info("‚úÖ EasyOCR ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        except ImportError:
            self.logger.warning("‚ö†Ô∏è EasyOCR ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á")
            self.easyocr = None

    async def _load_thai_tokenizer(self):
        """‡πÇ‡∏´‡∏•‡∏î Thai Tokenizer"""
        try:
            if self.pythainlp:
                from pythainlp.tokenize import word_tokenize
                self.thai_tokenizer = word_tokenize
                self.logger.info("‚úÖ Thai Tokenizer ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            else:
                self.thai_tokenizer = None
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Thai Tokenizer: {e}")
            self.thai_tokenizer = None

    async def translate_command(self, thai_command: str) -> str:
        """‡πÅ‡∏õ‡∏•‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©"""
        try:
            self.logger.info(f"üáπüá≠ ‡πÅ‡∏õ‡∏•‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÑ‡∏ó‡∏¢: {thai_command}")
            
            english_command = thai_command
            
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
            for thai_word, english_word in self.command_mappings.items():
                english_command = english_command.replace(thai_word, english_word)
            
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ element
            for thai_desc, english_desc in self.element_descriptions.items():
                english_command = english_command.replace(thai_desc, english_desc)
            
            self.logger.info(f"‚úÖ ‡πÅ‡∏õ‡∏•‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {english_command}")
            return english_command
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: {e}")
            return thai_command

    async def analyze_text(self, texts: List[str]) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢"""
        try:
            self.logger.info(f"üîç ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢: {len(texts)} texts")
            
            analysis_result = {
                'total_texts': len(texts),
                'thai_texts': [],
                'english_texts': [],
                'mixed_texts': [],
                'keywords': [],
                'sentiment': 'neutral',
                'timestamp': datetime.now().isoformat()
            }
            
            for text in texts:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤
                if self._is_thai_text(text):
                    analysis_result['thai_texts'].append(text)
                elif self._is_english_text(text):
                    analysis_result['english_texts'].append(text)
                else:
                    analysis_result['mixed_texts'].append(text)
                
                # ‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                keywords = await self._extract_keywords(text)
                analysis_result['keywords'].extend(keywords)
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment
            analysis_result['sentiment'] = await self._analyze_sentiment(texts)
            
            self.logger.info("‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {e}")
            return {}

    def _is_thai_text(self, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        thai_chars = re.findall(r'[\u0E00-\u0E7F]', text)
        return len(thai_chars) > len(text) * 0.3

    def _is_english_text(self, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        english_chars = re.findall(r'[a-zA-Z]', text)
        return len(english_chars) > len(text) * 0.5

    async def _extract_keywords(self, text: str) -> List[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        try:
            keywords = []
            
            if self.thai_tokenizer and self._is_thai_text(text):
                # ‡πÉ‡∏ä‡πâ Thai tokenizer
                tokens = self.thai_tokenizer(text)
                keywords = [token for token in tokens if len(token) > 1]
            else:
                # ‡πÉ‡∏ä‡πâ regex ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
                words = re.findall(r'\b\w+\b', text.lower())
                keywords = [word for word in words if len(word) > 2]
            
            return keywords[:10]  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 10 ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {e}")
            return []

    async def _analyze_sentiment(self, texts: List[str]) -> str:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        try:
            positive_words = ['‡∏î‡∏µ', '‡∏î‡∏µ‡πÉ‡∏à', '‡∏¢‡∏¥‡∏ô‡∏î‡∏µ', '‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì', '‡∏ä‡∏≠‡∏ö', '‡∏î‡∏µ‡∏°‡∏≤‡∏Å', '‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°']
            negative_words = ['‡πÑ‡∏°‡πà‡∏î‡∏µ', '‡πÅ‡∏¢‡πà', '‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à', '‡πÇ‡∏Å‡∏£‡∏ò', '‡πÑ‡∏°‡πà‡∏ä‡∏≠‡∏ö', '‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å', '‡∏ô‡πà‡∏≤‡πÄ‡∏ö‡∏∑‡πà‡∏≠']
            
            positive_count = 0
            negative_count = 0
            
            for text in texts:
                for word in positive_words:
                    if word in text:
                        positive_count += 1
                
                for word in negative_words:
                    if word in text:
                        negative_count += 1
            
            if positive_count > negative_count:
                return 'positive'
            elif negative_count > positive_count:
                return 'negative'
            else:
                return 'neutral'
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment: {e}")
            return 'neutral'

    async def extract_thai_text(self, image_path: str) -> List[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"""
        try:
            self.logger.info(f"üáπüá≠ ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å: {image_path}")
            
            if not self.easyocr:
                self.logger.warning("‚ö†Ô∏è EasyOCR ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
                return []
            
            # ‡πÉ‡∏ä‡πâ EasyOCR ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            results = self.easyocr.readtext(image_path)
            
            thai_texts = []
            for (bbox, text, confidence) in results:
                if confidence > 0.5:  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô > 50%
                    thai_texts.append(text)
            
            self.logger.info(f"‚úÖ ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(thai_texts)} texts")
            return thai_texts
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢: {e}")
            return []

    async def analyze_screenshot_content(self, texts: List[str]) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á screenshot"""
        try:
            self.logger.info(f"üîç ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ screenshot: {len(texts)} texts")
            
            analysis = {
                'total_texts': len(texts),
                'thai_texts': [],
                'english_texts': [],
                'buttons': [],
                'links': [],
                'forms': [],
                'headings': [],
                'content': [],
                'timestamp': datetime.now().isoformat()
            }
            
            for text in texts:
                # ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                if self._is_button_text(text):
                    analysis['buttons'].append(text)
                elif self._is_link_text(text):
                    analysis['links'].append(text)
                elif self._is_form_text(text):
                    analysis['forms'].append(text)
                elif self._is_heading_text(text):
                    analysis['headings'].append(text)
                else:
                    analysis['content'].append(text)
                
                # ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏†‡∏≤‡∏©‡∏≤
                if self._is_thai_text(text):
                    analysis['thai_texts'].append(text)
                elif self._is_english_text(text):
                    analysis['english_texts'].append(text)
            
            self.logger.info("‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ screenshot ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
            return analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {e}")
            return {}

    def _is_button_text(self, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏∏‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        button_keywords = ['‡∏õ‡∏∏‡πà‡∏°', 'button', '‡∏Ñ‡∏•‡∏¥‡∏Å', 'click', '‡∏™‡πà‡∏á', 'submit', '‡∏ï‡∏Å‡∏•‡∏á', 'ok', '‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å', 'cancel']
        return any(keyword in text.lower() for keyword in button_keywords)

    def _is_link_text(self, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        link_keywords = ['‡∏•‡∏¥‡∏á‡∏Å‡πå', 'link', 'http', 'www', '.com', '.th']
        return any(keyword in text.lower() for keyword in link_keywords)

    def _is_form_text(self, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        form_keywords = ['‡∏ü‡∏≠‡∏£‡πå‡∏°', 'form', '‡∏Å‡∏£‡∏≠‡∏Å', 'fill', '‡∏ä‡∏∑‡πà‡∏≠', 'name', '‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô', 'password', '‡∏≠‡∏µ‡πÄ‡∏°‡∏•', 'email']
        return any(keyword in text.lower() for keyword in form_keywords)

    def _is_heading_text(self, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
        return len(text) < 50 and (text.isupper() or text[0].isupper())

    async def process_natural_command(self, command: str) -> Dict[str, Any]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        try:
            self.logger.info(f"üáπüá≠ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥: {command}")
            
            # ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á
            tokens = await self._tokenize_command(command)
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á
            command_structure = await self._analyze_command_structure(tokens)
            
            # ‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à
            system_command = await self._convert_to_system_command(command_structure)
            
            result = {
                'original_command': command,
                'tokens': tokens,
                'structure': command_structure,
                'system_command': system_command,
                'confidence': 0.8,
                'timestamp': datetime.now().isoformat()
            }
            
            self.logger.info("‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: {e}")
            return {}

    async def _tokenize_command(self, command: str) -> List[str]:
        """‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô tokens"""
        try:
            if self.thai_tokenizer:
                return self.thai_tokenizer(command)
            else:
                return command.split()
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Tokenize command: {e}")
            return command.split()

    async def _analyze_command_structure(self, tokens: List[str]) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á"""
        try:
            structure = {
                'action': None,
                'target': None,
                'parameters': {},
                'modifiers': []
            }
            
            for i, token in enumerate(tokens):
                # ‡∏´‡∏≤ action
                if token in ['‡πÄ‡∏õ‡∏¥‡∏î', '‡∏õ‡∏¥‡∏î', '‡∏Ñ‡∏•‡∏¥‡∏Å', '‡∏Å‡∏£‡∏≠‡∏Å', '‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤', '‡∏£‡∏≠', '‡∏ñ‡πà‡∏≤‡∏¢', '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå']:
                    structure['action'] = token
                
                # ‡∏´‡∏≤ target
                elif token in ['‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå', '‡∏õ‡∏∏‡πà‡∏°', '‡∏ü‡∏≠‡∏£‡πå‡∏°', '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°', '‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û', '‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠']:
                    structure['target'] = token
                
                # ‡∏´‡∏≤ parameters
                elif '=' in token:
                    key, value = token.split('=', 1)
                    structure['parameters'][key] = value
                
                # ‡∏´‡∏≤ modifiers
                else:
                    structure['modifiers'].append(token)
            
            return structure
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Analyze command structure: {e}")
            return {}

    async def _convert_to_system_command(self, structure: Dict[str, Any]) -> Dict[str, Any]:
        """‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏£‡∏∞‡∏ö‡∏ö"""
        try:
            system_command = {
                'method': None,
                'args': {},
                'kwargs': {}
            }
            
            # ‡πÅ‡∏õ‡∏•‡∏á action ‡πÄ‡∏õ‡πá‡∏ô method
            action_mapping = {
                '‡πÄ‡∏õ‡∏¥‡∏î': 'navigate_to',
                '‡∏õ‡∏¥‡∏î': 'close_browser',
                '‡∏Ñ‡∏•‡∏¥‡∏Å': 'click_element',
                '‡∏Å‡∏£‡∏≠‡∏Å': 'fill_field',
                '‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤': 'search',
                '‡∏£‡∏≠': 'wait_for',
                '‡∏ñ‡πà‡∏≤‡∏¢': 'take_screenshot',
                '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå': 'analyze_page'
            }
            
            if structure.get('action') in action_mapping:
                system_command['method'] = action_mapping[structure['action']]
            
            # ‡πÅ‡∏õ‡∏•‡∏á parameters
            system_command['args'] = structure.get('parameters', {})
            
            return system_command
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Convert to system command: {e}")
            return {}

    def get_status(self):
        """‡∏£‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞"""
        return {
            'initialized': self.initialized,
            'pythainlp_loaded': self.pythainlp is not None,
            'easyocr_loaded': self.easyocr is not None,
            'tokenizer_loaded': self.thai_tokenizer is not None,
            'command_mappings': len(self.command_mappings),
            'element_descriptions': len(self.element_descriptions),
            'timestamp': datetime.now().isoformat()
        }

# Test Function
async def test_thai_processor():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö Thai Language Processor"""
    print("üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö WAWAGOD Thai Language Processor...")
    
    processor = WAWAGODThaiProcessor()
    
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    success = await processor.initialize()
    if not success:
        print("‚ùå ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß")
        return
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á
    thai_command = "‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå Google ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"
    english_command = await processor.translate_command(thai_command)
    print(f"üáπüá≠ ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÑ‡∏ó‡∏¢: {thai_command}")
    print(f"üá∫üá∏ ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©: {english_command}")
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    texts = ["‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö", "Hello world", "‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤", "‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"]
    analysis = await processor.analyze_text(texts)
    print(f"üìä ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {analysis}")
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
    natural_command = "‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå Google ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"
    result = await processor.process_natural_command(natural_command)
    print(f"üéØ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {result}")
    
    print("‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

if __name__ == "__main__":
    asyncio.run(test_thai_processor()) 