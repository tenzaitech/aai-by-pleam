# WAWAGOT.AI - GPU Manager
# ===============================================================================
# WAWAGOT.AI - ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ GPU ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
# ===============================================================================
# Created: 2024-12-19
# Purpose: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ GPU acceleration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI/ML workloads
# ===============================================================================

import os
import sys
import json
import asyncio
import threading
import time
from datetime import datetime
try:
    from zoneinfo import ZoneInfo
    TZ_BANGKOK = ZoneInfo("Asia/Bangkok")
except ImportError:
    import pytz
    TZ_BANGKOK = pytz.timezone("Asia/Bangkok")
from typing import Dict, List, Any, Optional, Tuple
import subprocess
import psutil

# GPU Libraries
try:
    import torch
    import torch.cuda as cuda
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import tensorflow as tf
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False

# ===============================================================================
# GPU DETECTION & MANAGEMENT
# ===============================================================================

class GPUManager:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ GPU ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô"""
    
    def __init__(self):
        self.gpu_info = {}
        self.gpu_processes = {}
        self.gpu_models = {}
        self.monitoring_active = False
        self.monitoring_thread = None
        self.gpu_utilization = {}
        self.gpu_memory = {}
        self.gpu_temperature = {}
        
    async def initialize_gpu_system(self) -> Dict[str, Any]:
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö GPU"""
        print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö GPU Manager...")
        
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU Hardware
            gpu_hardware = await self._detect_gpu_hardware()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU Libraries
            gpu_libraries = await self._check_gpu_libraries()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CUDA
            cuda_info = await self._check_cuda_support()
            
            # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô GPU Monitoring
            await self._start_gpu_monitoring()
            
            return {
                "success": True,
                "message": "GPU System ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à",
                "hardware": gpu_hardware,
                "libraries": gpu_libraries,
                "cuda": cuda_info,
                "monitoring": "active"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô GPU: {e}"
            }
    
    async def _detect_gpu_hardware(self) -> Dict[str, Any]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU Hardware"""
        print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU Hardware...")
        
        gpu_info = {
            "nvidia": [],
            "amd": [],
            "intel": [],
            "total_gpus": 0
        }
        
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö NVIDIA GPU
            if TORCH_AVAILABLE and cuda.is_available():
                nvidia_count = cuda.device_count()
                gpu_info["total_gpus"] += nvidia_count
                
                for i in range(nvidia_count):
                    gpu_props = cuda.get_device_properties(i)
                    gpu_info["nvidia"].append({
                        "id": i,
                        "name": gpu_props.name,
                        "memory_total": gpu_props.total_memory // 1024 // 1024 // 1024,  # GB
                        "compute_capability": f"{gpu_props.major}.{gpu_props.minor}",
                        "multiprocessor_count": gpu_props.multi_processor_count
                    })
                    print(f"   ‚úÖ NVIDIA GPU {i}: {gpu_props.name} ({gpu_props.total_memory // 1024 // 1024 // 1024} GB)")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU ‡∏ú‡πà‡∏≤‡∏ô nvidia-smi (Windows)
            try:
                result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,temperature.gpu,utilization.gpu', '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    lines = result.stdout.strip().split('\n')
                    for i, line in enumerate(lines):
                        if line.strip():
                            parts = line.split(', ')
                            if len(parts) >= 4:
                                gpu_info["nvidia"][i].update({
                                    "current_memory": int(parts[1]) // 1024,  # GB
                                    "temperature": int(parts[2]),
                                    "utilization": int(parts[3])
                                })
            except Exception as e:
                print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô nvidia-smi: {e}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö AMD GPU (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            try:
                # AMD ROCm ‡∏´‡∏£‡∏∑‡∏≠ OpenCL
                pass
            except Exception:
                pass
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Intel GPU (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            try:
                # Intel oneAPI ‡∏´‡∏£‡∏∑‡∏≠ OpenCL
                pass
            except Exception:
                pass
            
            return gpu_info
            
        except Exception as e:
            print(f"   ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU: {e}")
            return gpu_info
    
    async def _check_gpu_libraries(self) -> Dict[str, Any]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU Libraries"""
        print("üì¶ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU Libraries...")
        
        libraries = {
            "pytorch": {
                "available": TORCH_AVAILABLE,
                "cuda_available": False,
                "version": None,
                "cuda_version": None
            },
            "tensorflow": {
                "available": TENSORFLOW_AVAILABLE,
                "gpu_available": False,
                "version": None
            },
            "opencv": {
                "available": OPENCV_AVAILABLE,
                "cuda_available": False,
                "version": None
            }
        }
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö PyTorch
        if TORCH_AVAILABLE:
            libraries["pytorch"]["version"] = torch.__version__
            libraries["pytorch"]["cuda_available"] = cuda.is_available()
            if cuda.is_available():
                libraries["pytorch"]["cuda_version"] = cuda.get_device_name(0)
                print(f"   ‚úÖ PyTorch {torch.__version__} + CUDA")
            else:
                print(f"   ‚ö†Ô∏è PyTorch {torch.__version__} (CPU only)")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö TensorFlow
        if TENSORFLOW_AVAILABLE:
            libraries["tensorflow"]["version"] = tf.__version__
            libraries["tensorflow"]["gpu_available"] = len(tf.config.list_physical_devices('GPU')) > 0
            if libraries["tensorflow"]["gpu_available"]:
                print(f"   ‚úÖ TensorFlow {tf.__version__} + GPU")
            else:
                print(f"   ‚ö†Ô∏è TensorFlow {tf.__version__} (CPU only)")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö OpenCV
        if OPENCV_AVAILABLE:
            libraries["opencv"]["version"] = cv2.__version__
            libraries["opencv"]["cuda_available"] = cv2.cuda.getCudaEnabledDeviceCount() > 0
            if libraries["opencv"]["cuda_available"]:
                print(f"   ‚úÖ OpenCV {cv2.__version__} + CUDA")
            else:
                print(f"   ‚ö†Ô∏è OpenCV {cv2.__version__} (CPU only)")
        
        return libraries
    
    async def _check_cuda_support(self) -> Dict[str, Any]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CUDA Support"""
        print("üîß ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CUDA Support...")
        
        cuda_info = {
            "cuda_available": False,
            "cuda_version": None,
            "cudnn_version": None,
            "driver_version": None
        }
        
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CUDA ‡∏ú‡πà‡∏≤‡∏ô PyTorch
            if TORCH_AVAILABLE and cuda.is_available():
                cuda_info["cuda_available"] = True
                cuda_info["cuda_version"] = torch.version.cuda
                print(f"   ‚úÖ CUDA {torch.version.cuda} available")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CUDA ‡∏ú‡πà‡∏≤‡∏ô nvidia-smi
            try:
                result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    # Parse driver version
                    for line in result.stdout.split('\n'):
                        if 'Driver Version' in line:
                            driver_version = line.split('Driver Version: ')[1].split()[0]
                            cuda_info["driver_version"] = driver_version
                            print(f"   ‚úÖ NVIDIA Driver: {driver_version}")
                            break
            except Exception as e:
                print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö NVIDIA Driver: {e}")
            
            return cuda_info
            
        except Exception as e:
            print(f"   ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CUDA: {e}")
            return cuda_info

# ===============================================================================
# GPU MONITORING
# ===============================================================================

    async def _start_gpu_monitoring(self):
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô GPU Monitoring"""
        if not self.monitoring_active:
            self.monitoring_active = True
            self.monitoring_thread = threading.Thread(target=self._gpu_monitoring_loop)
            self.monitoring_thread.daemon = True
            self.monitoring_thread.start()
            print("üìä ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô GPU Monitoring")
    
    def stop_gpu_monitoring(self):
        """‡∏´‡∏¢‡∏∏‡∏î GPU Monitoring"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join()
        print("üõë ‡∏´‡∏¢‡∏∏‡∏î GPU Monitoring")
    
    def _gpu_monitoring_loop(self):
        """‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° GPU"""
        while self.monitoring_active:
            try:
                asyncio.run(self._update_gpu_status())
                time.sleep(5)  # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏ó‡∏∏‡∏Å 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
            except Exception as e:
                print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° GPU: {e}")
                time.sleep(10)
    
    async def _update_gpu_status(self):
        """‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ GPU"""
        try:
            # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó GPU utilization
            if TORCH_AVAILABLE and cuda.is_available():
                for i in range(cuda.device_count()):
                    # ‡πÉ‡∏ä‡πâ nvidia-smi ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• real-time
                    try:
                        result = subprocess.run([
                            'nvidia-smi', 
                            '--query-gpu=index,utilization.gpu,memory.used,memory.total,temperature.gpu',
                            '--format=csv,noheader,nounits',
                            '-i', str(i)
                        ], capture_output=True, text=True, timeout=5)
                        
                        if result.returncode == 0:
                            parts = result.stdout.strip().split(', ')
                            if len(parts) >= 5:
                                self.gpu_utilization[i] = {
                                    "gpu_usage": int(parts[1]),
                                    "memory_used": int(parts[2]) // 1024,  # GB
                                    "memory_total": int(parts[3]) // 1024,  # GB
                                    "temperature": int(parts[4]),
                                    "timestamp": datetime.now(TZ_BANGKOK).isoformat()
                                }
                    except Exception:
                        pass
                        
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ GPU: {e}")

# ===============================================================================
# GPU OPERATIONS
# ===============================================================================

    async def get_gpu_status(self) -> Dict[str, Any]:
        """‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ GPU ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        try:
            return {
                "success": True,
                "timestamp": datetime.now(TZ_BANGKOK).isoformat(),
                "hardware": self.gpu_info,
                "utilization": self.gpu_utilization,
                "libraries": await self._check_gpu_libraries(),
                "cuda": await self._check_cuda_support(),
                "monitoring_active": self.monitoring_active,
                "total_gpus": len(self.gpu_utilization)
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def optimize_gpu_usage(self) -> Dict[str, Any]:
        """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ GPU"""
        try:
            optimizations = []
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU memory
            for gpu_id, status in self.gpu_utilization.items():
                memory_usage = (status["memory_used"] / status["memory_total"]) * 100
                
                if memory_usage > 90:
                    optimizations.append({
                        "type": "memory_warning",
                        "gpu_id": gpu_id,
                        "message": f"GPU {gpu_id} memory usage: {memory_usage:.1f}%"
                    })
                
                if status["temperature"] > 80:
                    optimizations.append({
                        "type": "temperature_warning",
                        "gpu_id": gpu_id,
                        "message": f"GPU {gpu_id} temperature: {status['temperature']}¬∞C"
                    })
            
            # ‡∏•‡πâ‡∏≤‡∏á GPU cache ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            if TORCH_AVAILABLE and cuda.is_available():
                cuda.empty_cache()
                optimizations.append({
                    "type": "cache_cleared",
                    "message": "GPU cache cleared"
                })
            
            return {
                "success": True,
                "optimizations": optimizations
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def load_model_to_gpu(self, model_name: str, model_path: str) -> Dict[str, Any]:
        """‡πÇ‡∏´‡∏•‡∏î Model ‡πÑ‡∏õ‡∏¢‡∏±‡∏á GPU"""
        try:
            if not TORCH_AVAILABLE or not cuda.is_available():
                return {"success": False, "error": "GPU not available"}
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU memory
            gpu_memory_available = cuda.get_device_properties(0).total_memory - cuda.memory_allocated(0)
            
            # ‡πÇ‡∏´‡∏•‡∏î model (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)
            # model = torch.load(model_path, map_location='cuda:0')
            # model.eval()
            
            self.gpu_models[model_name] = {
                "path": model_path,
                "gpu_id": 0,
                "memory_used": 0,  # ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏£‡∏¥‡∏á
                "loaded_at": datetime.now(TZ_BANGKOK).isoformat()
            }
            
            return {
                "success": True,
                "message": f"Model {model_name} loaded to GPU",
                "gpu_memory_available": gpu_memory_available // 1024 // 1024 // 1024  # GB
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}

def get_hybrid_device(prefer_gpu: bool = True) -> str:
    """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å device ‡πÅ‡∏ö‡∏ö hybrid (GPU/CPU)"""
    try:
        import torch
        if prefer_gpu and torch.cuda.is_available():
            return 'cuda'
        return 'cpu'
    except ImportError:
        return 'cpu'

# ===============================================================================
# GPU-ACCELERATED SERVICES
# ===============================================================================

class GPUAcceleratedServices:
    """‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ GPU acceleration ‡πÅ‡∏ö‡∏ö hybrid"""
    
    def __init__(self, gpu_manager: GPUManager):
        self.gpu_manager = gpu_manager
        self.ai_models = {}
        self.processing_queue = asyncio.Queue()
    
    async def initialize_services(self) -> Dict[str, Any]:
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ GPU-accelerated ‡πÅ‡∏ö‡∏ö hybrid"""
        try:
            services = {
                "text_generation": await self._init_text_generation(),
                "image_processing": await self._init_image_processing(),
                "voice_synthesis": await self._init_voice_synthesis(),
                "ocr_processing": await self._init_ocr_processing()
            }
            
            return {
                "success": True,
                "services": services,
                "hybrid_mode": True
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _init_text_generation(self) -> Dict[str, Any]:
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Text Generation Service"""
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö OpenAI/Gemini integration
            return {
                "available": True,
                "providers": ["openai", "gemini"],
                "gpu_accelerated": True
            }
        except Exception as e:
            return {"available": False, "error": str(e)}
    
    async def _init_image_processing(self) -> Dict[str, Any]:
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Image Processing Service"""
        try:
            if OPENCV_AVAILABLE and cv2.cuda.getCudaEnabledDeviceCount() > 0:
                return {
                    "available": True,
                    "operations": ["resize", "filter", "detection", "segmentation"],
                    "gpu_accelerated": True
                }
            else:
                return {
                    "available": True,
                    "operations": ["resize", "filter", "detection"],
                    "gpu_accelerated": False
                }
        except Exception as e:
            return {"available": False, "error": str(e)}
    
    async def _init_voice_synthesis(self) -> Dict[str, Any]:
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Voice Synthesis Service"""
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Retell.AI integration
            return {
                "available": True,
                "providers": ["retell", "openai_tts"],
                "gpu_accelerated": True
            }
        except Exception as e:
            return {"available": False, "error": str(e)}
    
    async def _init_ocr_processing(self) -> Dict[str, Any]:
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô OCR Processing Service"""
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö OCR libraries
            return {
                "available": True,
                "engines": ["tesseract", "paddleocr"],
                "gpu_accelerated": True
            }
        except Exception as e:
            return {"available": False, "error": str(e)}

    async def _generate_text_hybrid(self, request, device: str = 'auto') -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ö‡∏ö hybrid"""
        try:
            actual_device = get_hybrid_device(device == 'gpu' or (device == 'auto' and True))
            
            # ‡πÉ‡∏ä‡πâ GPU memory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö model loading ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô cuda
            if actual_device == 'cuda' and TORCH_AVAILABLE and cuda.is_available():
                # ‡∏¢‡πâ‡∏≤‡∏¢ model ‡πÑ‡∏õ GPU ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                pass
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ OpenAI API ‡∏´‡∏£‡∏∑‡∏≠ Gemini API
            if request.provider == "openai":
                result = await self._call_openai_api(request)
            elif request.provider == "gemini":
                result = await self._call_gemini_api(request)
            else:
                result = {"error": f"Unknown provider: {request.provider}"}
            
            return {
                "success": True,
                "text": result.get("text", ""),
                "provider": request.provider,
                "model": request.model,
                "device_used": actual_device,
                "gpu_accelerated": actual_device == 'cuda',
                "processing_time": 0.1
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _process_image_hybrid(self, image_data: bytes, operation: str, device: str = 'auto') -> Dict[str, Any]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö hybrid"""
        try:
            actual_device = get_hybrid_device(device == 'gpu' or (device == 'auto' and True))
            
            if OPENCV_AVAILABLE and actual_device == 'cuda':
                # ‡πÉ‡∏ä‡πâ OpenCV CUDA
                if cv2.cuda.getCudaEnabledDeviceCount() > 0:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numpy array
                    import numpy as np
                    nparr = np.frombuffer(image_data, np.uint8)
                    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                    
                    # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ GPU
                    gpu_img = cv2.cuda_GpuMat()
                    gpu_img.upload(img)
                    
                    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏≤‡∏° operation
                    if operation == "resize":
                        gpu_result = cv2.cuda.resize(gpu_img, (640, 480))
                        result_img = gpu_result.download()
                    elif operation == "filter":
                        gpu_result = cv2.cuda.GaussianBlur(gpu_img, (15, 15), 0)
                        result_img = gpu_result.download()
                    else:
                        result_img = img
                    
                    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô bytes
                    _, buffer = cv2.imencode('.jpg', result_img)
                    result_data = base64.b64encode(buffer).decode('utf-8')
                    
                    return {
                        "success": True,
                        "operation": operation,
                        "device_used": actual_device,
                        "gpu_accelerated": True,
                        "image_data": result_data,
                        "processing_time": 0.05
                    }
            
            # Fallback ‡πÑ‡∏õ CPU
            return {
                "success": True,
                "operation": operation,
                "device_used": "cpu",
                "gpu_accelerated": False,
                "message": "Using CPU fallback"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}

# ===============================================================================
# MAIN EXECUTION
# ===============================================================================

async def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    print("üöÄ WAWAGOT.AI GPU Manager")
    print("=" * 60)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á GPU Manager
    gpu_manager = GPUManager()
    
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
    init_result = await gpu_manager.initialize_gpu_system()
    print(json.dumps(init_result, ensure_ascii=False, indent=2))
    
    if init_result["success"]:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á GPU-accelerated services
        services = GPUAcceleratedServices(gpu_manager)
        services_result = await services.initialize_services()
        print("\nGPU Services:", json.dumps(services_result, ensure_ascii=False, indent=2))
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ GPU
        status = await gpu_manager.get_gpu_status()
        print("\nGPU Status:", json.dumps(status, ensure_ascii=False, indent=2))
    
    # ‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà
    print("\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° GPU... (‡∏Å‡∏î Ctrl+C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î)")
    try:
        await asyncio.sleep(30)
    except KeyboardInterrupt:
        print("\n‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô...")
        gpu_manager.stop_gpu_monitoring()

if __name__ == "__main__":
    asyncio.run(main()) 